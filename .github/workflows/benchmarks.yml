---
name: benchmark tests

on:
  schedule:
    - cron: '0 10 * * 3,0'
  workflow_dispatch:
    inputs:
      features:
        description: 'included features for bench'
        required: true
        default: 'postgres,spark,mysql,odbc,delta_lake,databricks,duckdb,sqlite'

env:
  FEATURES: ${{ github.event_name == 'schedule' && 'postgres,spark,mysql,odbc,delta_lake,databricks,duckdb,sqlite' || inputs.features }}

jobs:
  run-database-bench:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    services:
      mysql:
        image: ${{ (contains(inputs.features, 'mysql') || github.event_name == 'schedule') && 'ghcr.io/spiceai/spice-mysql-bench:latest' || '' }}
        options: >-
          --health-cmd="mysqladmin ping -uroot -proot --silent"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 3306:3306
        env:
          MYSQL_ROOT_PASSWORD: root
      postgres:
        image: ${{ (contains(inputs.features, 'postgres') || github.event_name == 'schedule') && 'ghcr.io/spiceai/spice-postgres-bench:latest' || '' }}
        options: >-
          --shm-size=2gb
          --health-cmd="pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
        env:
          POSTGRES_PASSWORD: postgres
    steps:
      - uses: actions/checkout@v4

      - name: Set up Rust
        uses: ./.github/actions/setup-rust
        with:
          os: 'linux'

      - name: Set up Spice.ai API Key
        run: |
          echo 'SPICEAI_API_KEY="${{ secrets.SPICE_SECRET_SPICEAI_BENCHMARK_KEY }}"' > .env

      - name: Install Protoc
        if: contains(env.FEATURES, 'spark')
        uses: arduino/setup-protoc@v3

      - name: Install Databricks ODBC driver
        if: contains(env.FEATURES, 'odbc')
        run: |
          sudo apt-get install unixodbc unixodbc-dev unzip libsasl2-modules-gssapi-mit -y
          wget https://databricks-bi-artifacts.s3.us-east-2.amazonaws.com/simbaspark-drivers/odbc/2.8.2/SimbaSparkODBC-2.8.2.1013-Debian-64bit.zip
          unzip SimbaSparkODBC-2.8.2.1013-Debian-64bit.zip
          sudo dpkg -i simbaspark_2.8.2.1013-2_amd64.deb

      - name: Install Athena ODBC driver
        if: contains(env.FEATURES, 'odbc')
        run: |
          sudo apt-get install alien -y
          wget https://downloads.athena.us-east-1.amazonaws.com/drivers/ODBC/v2.0.3.0/Linux/AmazonAthenaODBC-2.0.3.0.rpm
          sudo alien -i AmazonAthenaODBC-2.0.3.0.rpm

      - run: cargo bench -p runtime --features ${{ env.FEATURES }}
        env:
          UPLOAD_RESULTS_DATASET: 'spiceai.tests.oss_benchmarks'
          PG_BENCHMARK_PG_HOST: localhost
          PG_BENCHMARK_PG_USER: postgres
          PG_BENCHMARK_PG_PASS: postgres
          PG_BENCHMARK_PG_SSLMODE: disable
          PG_BENCHMARK_PG_DBNAME: tpch_sf1
          SPICE_SPARK_REMOTE: ${{ secrets.SPICE_SPARK_REMOTE }}
          MYSQL_BENCHMARK_MYSQL_HOST: localhost
          MYSQL_BENCHMARK_MYSQL_USER: root
          MYSQL_BENCHMARK_MYSQL_PASS: root
          MYSQL_BENCHMARK_MYSQL_DB: tpch_sf1
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_ODBC_PATH: ${{ secrets.DATABRICKS_ODBC_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          AWS_DATABRICKS_DELTA_ACCESS_KEY_ID: ${{ secrets.AWS_DATABRICKS_DELTA_ACCESS_KEY_ID }}
          AWS_DATABRICKS_DELTA_SECRET_ACCESS_KEY: ${{ secrets.AWS_DATABRICKS_DELTA_SECRET_ACCESS_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_S3_ATHENA_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_S3_ATHENA_SECRET_ACCESS_KEY }}
