---
name: benchmark tests

on:
  schedule:
    - cron: '0 10 * * 3,0'
  workflow_dispatch:
    inputs:
      run_all:
        description: 'Run all benchmarks'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'
      selected_benchmark:
        description: 'Individual connector/accelerator benchmarks to run'
        required: false
        default: ''
        type: choice
        options:
          - ''
          - 'spice.ai connector'
          - 's3 connector'
          - 'spark connector'
          - 'postgres connector'
          - 'mysql connector'
          - 'odbc-databricks connector'
          - 'odbc-athena connector'
          - 'delta_lake connector'
          - 'arrow accelerator (memory mode)'
          - 'duckdb accelerator (memory mode)'
          - 'duckdb accelerator (file mode)'
          - 'sqlite accelerator (memory mode)'
          - 'sqlite accelerator (file mode)'
          - 'postgres accelerator'

env:
  FEATURES: 'postgres,spark,mysql,odbc,delta_lake,databricks,duckdb,sqlite'

jobs:
  build-database-bench-binary:
    name: Build Benchmark Test Binary
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Rust
        uses: ./.github/actions/setup-rust
        with:
          os: 'linux'

      - name: Install Protoc
        uses: arduino/setup-protoc@v3

      - name: Build benchmark binary
        run: cargo bench -p runtime --features ${{ env.FEATURES }} --profile release --no-run

      - name: Find, move, and rename benchmark binary
        run: find target/release/deps -type f -name "bench-*" ! -name "*.d" -exec mv {} ./spice_bench \;

      - name: Upload benchmark binary
        uses: actions/upload-artifact@v4
        with:
          name: spice_bench
          path: ./spice_bench

  run-database-bench:
    name: Run Connector/Accelerator Benchmark
    runs-on: ubuntu-latest
    needs: build-database-bench-binary

    services:
      mysql:
        image: ${{ matrix.cmd == '-c mysql' && 'ghcr.io/spiceai/spice-mysql-bench:latest' || '' }}
        options: >-
          --health-cmd="mysqladmin ping -uroot -proot --silent"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 3306:3306
        env:
          MYSQL_ROOT_PASSWORD: root
      postgres_conn:
        image: ${{ matrix.cmd == '-c postgres' && 'ghcr.io/spiceai/spice-postgres-bench:latest' || '' }}
        options: >-
          --shm-size=2gb
          --health-cmd="pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
        env:
          POSTGRES_PASSWORD: postgres
      postgres_acc:
        image: ${{ matrix.cmd == '-a postgres' && 'ghcr.io/cloudnative-pg/postgresql:16-bookworm' || '' }}
        options: >-
          --shm-size=2gb
          --health-cmd="pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
        env:
          POSTGRES_PASSWORD: postgres

    strategy:
      matrix:
        include:
          - cmd: '-c spice.ai'
            name: 'spice.ai connector'
          - cmd: '-c s3'
            name: 's3 connector'
          - cmd: '-c spark'
            name: 'spark connector'
          - cmd: '-c postgres'
            name: 'postgres connector'
          - cmd: '-c mysql'
            name: 'mysql connector'
          - cmd: '-c odbc-databricks'
            name: 'odbc-databricks connector'
          - cmd: '-c odbc-athena'
            name: 'odbc-athena connector'
          - cmd: '-c delta_lake'
            name: 'delta_lake connector'
          - cmd: '-a arrow'
            name: 'arrow accelerator (memory mode)'
          - cmd: '-a duckdb -m memory'
            name: 'duckdb accelerator (memory mode)'
          - cmd: '-a duckdb -m file'
            name: 'duckdb accelerator (memfileory mode)'
          - cmd: '-a sqlite -m memory'
            name: 'sqlite accelerator (memory mode)'
          - cmd: '-a sqlite -m file'
            name: 'sqlite accelerator (memory mode)'
          - cmd: '-a postgres'
            name: 'postgres accelerator'

    steps:
      - name: Checkout repository
        if: github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        uses: actions/checkout@v4

      - name: Set up Spice.ai API Key
        if: github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: |
          echo 'SPICEAI_API_KEY="${{ secrets.SPICE_SECRET_SPICEAI_BENCHMARK_KEY }}"' > .env

      - name: Download benchmark binary
        if: github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        uses: actions/download-artifact@v4
        with:
          name: spice_bench

      - name: 'Restart accelerator service container with customize configurations'
        if: matrix.cmd == '-a postgres' && (github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          docker exec ${{ job.services.postgres_acc.id }} sh -c 'echo "shared_buffers=1GB" >> /var/lib/postgresql/data/postgresql.conf'
          docker exec ${{ job.services.postgres_acc.id }} sh -c 'echo "work_mem=256MB" >> /var/lib/postgresql/data/postgresql.conf'
          docker exec ${{ job.services.postgres_acc.id }} sh -c 'echo "max_wal_size=6GB" >> /var/lib/postgresql/data/postgresql.conf'
          docker kill --signal=SIGHUP ${{ job.services.postgres_conn.id }}

      - name: Install Databricks ODBC driver
        if: matrix.cmd == '-c odbc-databricks' && (github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          sudo apt-get install unixodbc unixodbc-dev unzip libsasl2-modules-gssapi-mit -y
          wget https://databricks-bi-artifacts.s3.us-east-2.amazonaws.com/simbaspark-drivers/odbc/2.8.2/SimbaSparkODBC-2.8.2.1013-Debian-64bit.zip
          unzip SimbaSparkODBC-2.8.2.1013-Debian-64bit.zip
          sudo dpkg -i simbaspark_2.8.2.1013-2_amd64.deb

      - name: Install Athena ODBC driver
        if: matrix.cmd == '-c odbc-athena' && (github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule')
        run: |
          sudo apt-get install alien -y
          wget https://downloads.athena.us-east-1.amazonaws.com/drivers/ODBC/v2.0.3.0/Linux/AmazonAthenaODBC-2.0.3.0.rpm
          sudo alien -i AmazonAthenaODBC-2.0.3.0.rpm

      - name: Make benchmark binary executable
        if: github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: chmod +x ./spice_bench

      - name: Run benchmark with ${{ matrix.name }}
        if: github.event.inputs.selected_benchmark == matrix.name || github.event.inputs.run_all == 'true' || github.event_name == 'schedule'
        run: ./spice_bench --bench ${{ matrix.cmd }}
        continue-on-error: true
        env:
          UPLOAD_RESULTS_DATASET: 'spiceai.tests.oss_benchmarks'
          PG_BENCHMARK_PG_HOST: localhost
          PG_BENCHMARK_PG_USER: postgres
          PG_BENCHMARK_PG_PASS: postgres
          PG_BENCHMARK_PG_SSLMODE: disable
          PG_BENCHMARK_PG_DBNAME: tpch_sf1
          PG_BENCHMARK_ACC_PG_DBNAME: postgres
          SPICE_SPARK_REMOTE: ${{ secrets.SPICE_SPARK_REMOTE }}
          MYSQL_BENCHMARK_MYSQL_HOST: localhost
          MYSQL_BENCHMARK_MYSQL_USER: root
          MYSQL_BENCHMARK_MYSQL_PASS: root
          MYSQL_BENCHMARK_MYSQL_DB: tpch_sf1
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_ODBC_PATH: ${{ secrets.DATABRICKS_ODBC_PATH }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          AWS_DATABRICKS_DELTA_ACCESS_KEY_ID: ${{ secrets.AWS_DATABRICKS_DELTA_ACCESS_KEY_ID }}
          AWS_DATABRICKS_DELTA_SECRET_ACCESS_KEY: ${{ secrets.AWS_DATABRICKS_DELTA_SECRET_ACCESS_KEY }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_S3_ATHENA_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_S3_ATHENA_SECRET_ACCESS_KEY }}
